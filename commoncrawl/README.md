# Common Crawl
This is a Amazon dataset hosted in a public S3 bucket, but can also be retrieved via HTTPS links. Downloading from the HTTPS links means that the g-zipped files can be accessed without an AWS setup, but will lose the benefits of complimentary AWS services. The main dataset (CC-MAIN) also contains a sub-dataset (CC-NEWS) which includes only scraped webpages which are news articles.

The original aim was to extract the scraped webpages from New Zealand websites and apply sentiment analysis to the text to get an indication of online New Zealand sentiment over time.

---
## Contents:
1. CC-MAIN dataset structure
2. Overview of files/subdirectories
3. Explainer: retrieving CC-MAIN
4. Extra: CC-NEWS dataset structure
---


## 1. CC-MAIN dataset structure
The complete dataset structure is described at https://commoncrawl.org/the-data/get-started/. This dataset starts in 2012, but collected far less webpages in the first few years and the web-crawling configuration has changed a lot since 2012.

.warc = Web ARChive format. This is a filetype used to store scraped information from webpages ([ISO standard](https://www.iso.org/standard/68004.html)), with one file containing the HTML and other relevant metadata from *multiple* webpages. These are gzipped (compressed) before being stored in the _commoncrawl_ S3 bucket.

The ".warc.gz" files are released in batches about every 5 weeks. The batches are listed at https://index.commoncrawl.org/collinfo.json and use the naming convention CC-MAIN-YYYY-WW (YYYY = year, WW = week number). The paths for the ".warc.gz" files which contain the scraped information in each batch can be found at https://commoncrawl.s3.amazonaws.com/{BATCH_NAME}/warc.paths.gz or s3://commoncrawl/{BATCH_NAME}/warc.paths.gz. For example, the paths for the ".warc.gz" files in the CC-MAIN-2021-10 batch is available at https://commoncrawl.s3.amazonaws.com/CC-MAIN-2021-10/warc.paths.gz or s3://commoncrawl/CC-MAIN-2021-10/warc.paths.gz.

An example of an individual filepath is https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-10/segments/1614178347293.1/warc/CC-MAIN-20210224165708-20210224195708-00000.warc.gz OR 
s3://commoncrawl/crawl-data/CC-MAIN-2021-10/segments/1614178347293.1/warc/CC-MAIN-20210224165708-20210224195708-00000.warc.gz. This is the first file in the _CC-MAIN-2021-10_ batch and contains 44,408 scraped webpages, of which 98 have URLs with ".nz" as their top-level domain.


## 2. Overview of files/subdirectories
Most of the files in this part of the repo are very haphazard and probably difficult to re-use.

In the diagram below, anything with "[u]" was unversioned (as per gitignore) but not all unversioned files/subdirectories are included.
```
commoncrawl
├── __init__.py
├── README.md
│
├── cc-nz-articles-test.csv
├── cc_process.py
├── cc_process_v2.py
├── cc_bug_squashing.py
├── processed_ccmain/ [u]
├── processed_ccnews/ [u]
│
├── eda (coverage over time).ipynb
├── eda-test.ipynb
├── hedonometer_ccmain.ipynb
├── hedonometer_ccnews.ipynb
├── cc-nz-articles-test.csv [u]
├── hedonometer_words.json [u]
│
├── cost of Athena.ipynb
│
└── digital_economy
    ├── __init__.py
    ├── data/ [u]
    │   └── ecom_labels.json [exception]
    │
    ├── digital_economy.ipynb
    │
    ├── website_aggregate.py
    ├── post-cluster.ipynb
    │
    ├── ecom_word_cloud.ipynb
    ├── grid_search.py
    │
    ├── replicate_stats_nl.ipynb
    └── test_scraping.ipynb
```

The _cc_process.py_, _cc_process_v2.py_, and _cc_bug_squashing.py_ scripts are concerned with retrieving data from the CC-MAIN and CC-NEWS datasets. The V1 process downloaded via HTTPS links, can retrieve either CC-MAIN or CC-NEWS, and saved processed output to CSV files to local storage (_processed_ccnews/_ and _processed_ccmain/_ folders). The V2 process used S3 Select querying, downloaded via S3 links, can only retrieve CC-MAIN (explained why in next section), and saved processed output to CSV files in a S3 bucket.

The unversioned _cc-nz-articles-test.csv_ file was generated by running _cc_process.py_ to get 4 days of the CC-NEWS dataset. The unversioned _hedonometer_words.json_ file was generated by the two **hedonometer** notebooks and retrieved from [here](https://hedonometer.org/api/v1/words/?format=json&wordlist__title=labMT-en-v2).
The CSV was used in the _eda-test.ipynb_ and _hedonometer_ccnews.ipynb_ notebooks. Similar processed CC-NEWS output was used for the _eda (coverage over time).ipynb_ notebook. The _hedonometer_ccmain.ipynb_ notebook used CC-MAIN output that was generated by _cc_process_v2.py_ (and thus CSV files saved in S3 bucket). 
The two **hedonometer** notebooks aimed to replicate the hedonometer bag-of-words approach for inferring sentiment and the two **eda** notebooks were just used to explore the data collected.

The _cost of Athena.ipynb_ notebook was used to calculate the cost of querying the CC-MAIN index using Amazon Athena.

### Digital Economy
*Note:* This folder was dedicated to trying to leverage some insights about New Zealand websites (and their associated businesses) from the information in the commoncrawl dataset. Ultimately it was decided that the coverage of the commoncrawl dataset was too low (ie. only contained *some webpages* from 41,000 of the approx. 725,000 ".nz" websites), so a new git repository was started that aims to scrape every NZ website in real-time and then deduce various information. However, some of the work done in this folder is very useful for [that scrapy-based project](https://github.com/xaviermiles/creepy_crawlies).

The _digital_economy.ipynb_ notebook was the initial exploration of trying to use commoncrawl data for "digital economy".

The _website_aggregate.py_ script aggregates the commoncrawl output by website and applies K-Means clustering to the text on the websites.
The _post-cluster.ipynb_ notebook does some visualisations/analysis of the clusters generated using K-Means clustering.

The _data/ecom_labels.json_ file was **manually** constructed by visiting random ".nz" websites and **subjectively** judging whether they seemed like their websites could be used to purchase (or agree to purchase in the future) goods/services from a NZ business.
The _ecom_word_cloud.ipynb_ notebook was used to generate wordclouds from the text collected of websites that were marked "ecommerce". The _grid_search.py_ script performs grid searching to optimise various model configurations which are used to predict whether the websites in the JSON file were ecommerce or not.
The best predictions were made by TfIdf + SVM model, with an AUC score of 0.84 which indicates very reliable predictions.

The _replicate_stats_nl.ipynb_ notebook attempted to categorise websites using extracted keywords, similar to work previously done by Statistics Netherlands.
The _test_scraping.ipynb_ notebook attempted to identify shopping cart software used by websites using various HTML tags. This was supervised learning, with the labels generated using BuiltWith platform and manually inspecting HTML for consistent patterns for each shopping cart software.


## 3. Explainer: retrieving CC-MAIN
### V1 process
The _cc_process.py_ script supports retrieval of both the original dataset (CC-MAIN) and the subset which only includes news articles (CC-NEWS). This script extracts the scraped webpages with nz top-level domain (e.g. ".co.nz" or ".org.nz") by downloading the CC datasets using HTTPS links and then searching the WARC files for the relevant webpages, extracting these webpage's text, and saving this information to CSV files (along with the publish-datetime and webpage's URL). There is a separate CSV file produced for each '.warc.gz' file. This process works okay for CC-NEWS as this dataset is small-to-medium size (\~6,500 files for Jan 2020-Feb 2021), but is infeasible for CC-MAIN due the number of files required to be downloaded (\~700,000 files for Jan 2020-Feb 2021). During initial testing in the AWS environment, it seemed like it took _cc_process.py_ about 30 minutes per 100 files, so processing Jan 2020-Feb 2021 would take about 34 hours for CC-NEWS and about 150 days for CC-MAIN. Each '.warc.gz' file is 1 GB (before unzipping).

### V2 process
A more efficient process sends querys to data contained in the S3 bucket and only download the NZ webpages, since this would reduce the amount of data required to be downloaded and may provide more efficient processing. The _cc_process_v2.py_ script uses S3 Select to send queries to a collection of '.parquet' files (aka. index files) to find which parts of the commoncrawl WARC files contain '.nz' webpages and then only downloads these parts. (Only CC-MAIN has the set of index files, CC-NEWS does not so this process does not work for CC-NEWS.) 

The two main options for querying data in S3 buckets is _S3 Select_ and _Amazon Athena_. Both allow SQL-style queries to CSV, JSON, or [Parquet](https://databricks.com/glossary/what-is-parquet) datasets, 
which can be stored in a compressed format (GZIP typically recommended). Differences:
- Big Picture: S3 Select is more designed for ad hoc queries and Athena is designed more for big data
- Athena will incur larger costs and required activation
- Athena can perform queries on a collection of files (ie. an entire folder) whereas S3 Select sends a query to an individual file. This means Athena would be able to send a single query per CC-MAIN batch, while S3 Select must send a query per ".parquet" file which results in hundreds of queries (and corresponding responses) per CC-MAIN batch.
- S3 Select requires that "The maximum uncompressed row group size is 256MB". **This is not always satisfied for the CC-MAIN index files which this means that some of the files cannot be processed using S3 Select (rough estimate is 20% files cannot be processed).**

For the 2021-10 CC-MAIN batch there is about 300 index files, of which 65 (65/300 = 22%) were unable to be processed (but this seems to change between different runs on this batch). The '.nz' webpages seem to be extremely concentrated within a few index files; most index files contain zero '.nz' webpages while 'part-00108.parquet' contains 2.1 million '.nz' webpages (NB: it seems likely that 'part-00108.parquet' is the only index file which contains '.nz' webpages but this cannot be confirmed). There does not seem to be any documentation about how the indices are batched into the 300 files, so there is no (current) way to predict which index files contain '.nz' articles. The index files for this batch make up 202.8 GiB of data, which can be found by running (AWS CLI):
```
aws s3 ls commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2020-10/subset=warc/ --recursive --human-readable --summarize --no-sign-request
```

A possible way to see if many webpages have been missed is by looking at the number of articles over time and then comparing this to the number of news articles in GDELT over time. We will not truly know how many '.nz' articles we are missing when using S3 Select:
- worst-case scenario: all of them. This would be obvious since there would be a large gap in the data, but there would be no way to remedy this using S3 Select.
- best-case scenario: none of them.
- average-case scenario: probably all of them, since the '.nz' webpages seem to be extremely concentrated within a small number of '.parquet' files (ie. within one or two files).

NB: GDELT collection method takes all news articles which _mention_ New Zealand, where commoncrawl collection method takes all webpages which use New Zealand top-level domain (.nz).


## 4. Extra: CC-NEWS dataset structure
**NOTE THAT CC-NEWS has a different dataset structure, release schedule and collation frequency to CC-MAIN.**

Information about this dataset can be found at https://commoncrawl.org/2016/10/news-dataset-available/. It starts in 2016.

The ".warc.gz" files are released every hour or two, which usually results in 1 GB files.
The individual files are available at https://commoncrawl.s3.amazonaws.com/crawl-data/CC-NEWS/YYYY/MM/ or s3://commoncrawl/crawl-data/CC-NEWS/YYYY/MM/ (with YYYY = year, MM = month).

An example of an individual filepath is https://commoncrawl.s3.amazonaws.com/crawl-data/CC-NEWS/2021/01/CC-NEWS-20210101014736-01421.warc.gz or s3://commoncrawl/crawl-data/CC-NEWS/2021/01/CC-NEWS-20210101014736-01421.warc.gz. This is the first file for January 2021 and contains 34,277 scraped webpages, of which 58 have URLs with '.nz' as their top-level domain.

There is no "warc.path.gz" or index files generated for CC-NEWS.
