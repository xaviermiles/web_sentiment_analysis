{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assisted-exchange",
   "metadata": {},
   "source": [
    "# Country-wise comparison\n",
    "\n",
    "**V1.1 includes S3 integration, whereas the original notebook did not.**\n",
    "\n",
    "Comparison with New Zealand, Australia, and Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amended-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import boto3\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7, 8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scenic-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session(profile_name=\"xmiles\")\n",
    "\n",
    "def upload_to_s3(in_fpath, out_key):\n",
    "    s3 = sess.client('s3')\n",
    "    bucket_name = 'statsnz-covid-xmiles'\n",
    "    \n",
    "    s3.put_object(Body=open(in_fpath, 'rb'), Bucket=bucket_name, Key=out_key)\n",
    "    \n",
    "\n",
    "def read_file_from_s3(key):\n",
    "    s3 = sess.client('s3')\n",
    "    bucket_name = 'statsnz-covid-xmiles'\n",
    "    \n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    content = obj['Body'].read().decode()\n",
    "    \n",
    "    return content\n",
    "\n",
    "def list_all_objects_s3(bucket, prefix):\n",
    "    \"\"\"\n",
    "    Necessary since the list_objects_v2() function only lists the first 1000 \n",
    "    objects, and requires a continuation token to get the next 1000 objects.\n",
    "    \"\"\"\n",
    "    s3 = sess.client('s3')\n",
    "    keys = []\n",
    "    truncated = True\n",
    "    next_cont_token = \"\"\n",
    "\n",
    "    while truncated:\n",
    "        if next_cont_token:\n",
    "            resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, \n",
    "                                      ContinuationToken=next_cont_token)\n",
    "        else:\n",
    "            resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        keys += [x['Key'] for x in resp['Contents'] \n",
    "                 if \".ipynb_checkpoints\" not in x['Key']]\n",
    "\n",
    "        truncated = resp['IsTruncated']\n",
    "        if truncated:\n",
    "            next_cont_token = resp['NextContinuationToken']\n",
    "            \n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consecutive-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csvs(mergefile, infpaths, overwrite=False):\n",
    "    if os.path.exists(mergefile) and not overwrite:\n",
    "        return\n",
    "    \n",
    "    with open(mergefile, 'w', newline=\"\") as outfile:\n",
    "        outwriter = csv.writer(outfile, delimiter=',')\n",
    "        outwriter.writerow(headers)\n",
    "        for fpath in infpaths:\n",
    "            with open(fpath) as infile:\n",
    "                inwriter = csv.reader(infile, delimiter=',')\n",
    "                outwriter.writerows(inwriter)\n",
    "                \n",
    "\n",
    "def merge_csvs_from_s3(mergefile, inkeys, bucket, headers, overwrite=False):\n",
    "    if os.path.exists(mergefile) and not overwrite:\n",
    "        return\n",
    "    \n",
    "    # Clear existing CSV\n",
    "    with open(mergefile, 'w') as f:\n",
    "        f.write('')\n",
    "    \n",
    "    for i, key in enumerate(inkeys):\n",
    "        content = read_csv_from_s3(bucket, key)\n",
    "        with open(mergefile, mode='a') as f:\n",
    "            f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-metabolism",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = [\n",
    "    'gkg_id', 'date', 'source', 'source_name', 'doc_id', \n",
    "    'themes', 'locations', 'persons', 'orgs', \n",
    "    'tone', 'pos', 'neg', 'polarity', 'ard', 'srd',\n",
    "    'wc', \n",
    "    'lexicode_neg', 'lexicode_pos', # c3.*\n",
    "    'MACROECONOMICS', 'ENERGY', 'FISHERIES', \n",
    "    'TRANSPORTATION', 'CRIME', 'SOCIAL_WELFARE',\n",
    "    'HOUSING', 'FINANCE', 'DEFENCE', 'SSTC',\n",
    "    'FOREIGN_TRADE', 'CIVIL_RIGHTS', \n",
    "    'INTL_AFFAIRS', 'GOVERNMENT_OPS',\n",
    "    'LAND-WATER-MANAGEMENT', 'CULTURE',\n",
    "    'PROV_LOCAL', 'INTERGOVERNMENTAL',\n",
    "    'CONSTITUTIONAL_NATL_UNITY', 'ABORIGINAL',\n",
    "    'RELIGION', 'HEALTHCARE', 'AGRICULTURE',\n",
    "    'FORESTRY', 'LABOUR', 'IMMIGRATION',\n",
    "    'EDUCATION', 'ENVIRONMENT',\n",
    "    'finstab_pos', 'finstab_neg', 'finstab_neutral',\n",
    "    'finsent_neg', 'finsent_pos', 'finsent_unc',\n",
    "    'opin_neg', 'opin_pos',\n",
    "    'sent_pos', 'sent_neg', 'sent_pol'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sought-plate",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_to_prefix = { \n",
    "    'nz': \"processed_gdelt_nz/\",\n",
    "    'au': \"processed_gdelt_au/\",\n",
    "    'ca': \"processed_gdelt_ca/\"\n",
    "}\n",
    "countries = ['nz', 'au', 'ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-designer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_2020_and_2021_files(country):\n",
    "    files = [os.path.join(country_to_folder[country], file)\n",
    "             for file in os.listdir(country_to_folder[country])\n",
    "             if re.search(r'202[0-1]\\d{10}.gkg.csv', file)]\n",
    "    return sorted(files)\n",
    "\n",
    "# nz_files_20_21 = get_2020_and_2021_files(\"nz\")\n",
    "au_files_20_21 = get_2020_and_2021_files(\"au\")\n",
    "ca_files_20_21 = get_2020_and_2021_files(\"ca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_csvs('gdelt-nz-20-21.csv', nz_files_20_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_csvs('gdelt-au-20-21.csv', au_files_20_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_csvs('gdelt-ca-20-21.csv', ca_files_20_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-salad",
   "metadata": {},
   "source": [
    "**Loading the merged 2020/2021 CSV files for New Zealand, Australia, and Canada exceeds the available RAM so jupyter crashes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "union-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_aggregated_dfs(csv_file):\n",
    "    \"\"\"\n",
    "    Returns four DataFrames for the given country\n",
    "    - daily_tone: tone, pos, neg (daily)\n",
    "    - weekly_tone: tone, pos, neg (weekly)\n",
    "    - daily_count: number of articles (daily)\n",
    "    - weekly_count: number of articles (weekly)\n",
    "    \"\"\"\n",
    "    gdelt = pd.read_csv(csv_file)\n",
    "    print(csv_file)\n",
    "    print(\"read\")\n",
    "    \n",
    "    gdelt['date'] = pd.to_datetime(gdelt['date'], format=\"%Y%m%d%H%M%S\")\n",
    "    gdelt = gdelt.sort_values(by=[\"gkg_id\"]).reset_index()\n",
    "    print(\"tidied\")\n",
    "    \n",
    "    daily_tone = gdelt.resample('D', on='date')[['tone', 'pos', 'neg']].mean()\n",
    "    daily_count = gdelt.resample('D', on='date')['gkg_id'].count()\n",
    "    \n",
    "    weekly_tone = gdelt.resample('W-Mon', on='date')[['tone', 'pos', 'neg']].mean()\n",
    "    weekly_count = gdelt.resample('W-Mon', on='date')['gkg_id'].count()\n",
    "    # Remove partial weeks at beginning and end of weekly-aggregation\n",
    "#     first_monday = \n",
    "#     final_sunday = \n",
    "#     weekly_tone = weekly_tone[first_monday <= weekly_tone['date'] <= final_sunday]\n",
    "#     weekly_count = weekly_count[first_monday <= weekly_count['date'] <= final_sunday]\n",
    "    print(\"compiled\")\n",
    "    \n",
    "    return daily_tone, weekly_tone, daily_count, weekly_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-effects",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nz_dfs = get_time_aggregated_dfs('gdelt-nz-20-21.csv')\n",
    "au_dfs = get_time_aggregated_dfs('gdelt-au-20-21.csv')\n",
    "ca_dfs = get_time_aggregated_dfs('gdelt-ca-20-21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_columns = pd.MultiIndex.from_product([countries, ['tone', 'pos', 'neg']])\n",
    "\n",
    "daily_tone = pd.concat([nz_dfs[0], au_dfs[0], ca_dfs[0]], axis=1)\n",
    "daily_tone.columns = tone_columns\n",
    "\n",
    "weekly_tone = pd.concat([nz_dfs[1], au_dfs[1], ca_dfs[1]], axis=1)\n",
    "weekly_tone.columns = tone_columns\n",
    "\n",
    "daily_tone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_tone_long = daily_tone.xs('tone', axis=1, level=1) \\\n",
    "                            .reset_index() \\\n",
    "                            .melt(\"date\",\n",
    "                                  var_name=\"country\", \n",
    "                                  value_name=\"tone\", \n",
    "                                  value_vars=['nz','au','ca'])\n",
    "weekly_tone_long = weekly_tone.xs('tone', axis=1, level=1) \\\n",
    "                              .reset_index() \\\n",
    "                              .melt(\"date\",\n",
    "                                    var_name=\"country\", \n",
    "                                    value_name=\"tone\", \n",
    "                                    value_vars=['nz','au','ca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=daily_tone_long, x=\"date\", y=\"tone\", hue=\"country\")\n",
    "g.set(title=\"Daily tone of News (2020-present)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=weekly_tone_long, x=\"date\", y=\"tone\", hue=\"country\")\n",
    "g.set(title=\"Weekly tone of News (2020-present)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count = pd.concat({'nz': nz_dfs[2], 'au': au_dfs[2], 'ca': ca_dfs[2]}, axis=1)\n",
    "weekly_count = pd.concat({'nz': nz_dfs[3], 'au': au_dfs[3], 'ca': ca_dfs[3]}, axis=1)\n",
    "\n",
    "daily_count_long = daily_count.reset_index() \\\n",
    "                              .melt(\"date\",\n",
    "                                    var_name=\"country\", \n",
    "                                    value_name=\"num_articles\", \n",
    "                                    value_vars=['nz','au','ca'])\n",
    "weekly_count_long = weekly_count.reset_index() \\\n",
    "                                .melt(\"date\",\n",
    "                                      var_name=\"country\", \n",
    "                                      value_name=\"num_articles\", \n",
    "                                      value_vars=['nz','au','ca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=daily_count_long, x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Number of news articles (daily)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=weekly_count_long, x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Number of news articles (weekly)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-apparatus",
   "metadata": {},
   "source": [
    "## Verify that seasonality is weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count.index.strftime(\"%b %y\")\n",
    "['Jan 20', 'Mar 20', 'Jul 20', 'Oct 20', 'Jan 21', 'Mar 21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_trends = pd.concat({\n",
    "    country: seasonal_decompose(daily_count.dropna()[country],\n",
    "                                model='additive'\n",
    "                                ).trend\n",
    "    for country in countries\n",
    "}, axis=1)\n",
    "\n",
    "g = sns.lineplot(data=tone_trends, dashes=False)\n",
    "g.set(title=\"Daily number of articles - trend component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count['Day'] = daily_count.index.day_name().astype(\"category\").reorder_categories(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "first_monday = daily_count[daily_count['Day'] == \"Monday\"].index[0]\n",
    "first_monday_idx = (first_monday - datetime(2020, 1, 1)).days\n",
    "\n",
    "first_monday, first_monday_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_seasonals = pd.concat({\n",
    "    country: seasonal_decompose(daily_count.dropna()[country],#, 'tone'], \n",
    "                                model='additive'\n",
    "                                ).seasonal[first_monday_idx:(first_monday_idx+6)]\n",
    "    for country in countries\n",
    "}, axis=1)\n",
    "\n",
    "g = sns.lineplot(data=tone_seasonals, dashes=False)\n",
    "g.set(title=\"Daily number of articles - seasonal component (Mon - Sun)\",\n",
    "      xticks=[], xlabel='');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-twenty",
   "metadata": {},
   "source": [
    "# Check previous years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surprised-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_18_to_21_keys(country):\n",
    "    keys = [key\n",
    "            for key in list_all_objects_s3(\"statsnz-covid-xmiles\", \n",
    "                                           country_to_prefix[country])\n",
    "            if re.search(r'(201[8-9]|202[0-1])\\d{10}.gkg.csv', key)\n",
    "            ]\n",
    "    return sorted(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "missing-sarah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 228 ms, total: 1min 27s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nz_keys_18_21 = get_18_to_21_keys(\"nz\")\n",
    "au_keys_18_21 = get_18_to_21_keys(\"au\")\n",
    "ca_keys_18_21 = get_18_to_21_keys(\"ca\")\n",
    "# CPU time (user): 1min 27s\n",
    "# Wall time: 2min 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_csvs_from_s3('gdelt-nz-18-21.csv', nz_keys_18_21, \"statsnz-covid-xmiles\")\n",
    "# CPU time (user): 2min 37s\n",
    "# Wall time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_csvs_from_s3('gdelt-au-18-21.csv', au_keys_18_21, \"statsnz-covid-xmiles\")\n",
    "# CPU time (user): 4min 7s\n",
    "# Wall time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_csvs_from_s3('gdelt-ca-18-21.csv', ca_keys_18_21, \"statsnz-covid-xmiles\")\n",
    "# CPU time (user):\n",
    "# Wall time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for csvfile in ['gdelt-nz-18-21.csv', 'gdelt-au-18-21.csv', 'gdelt-ca-18-21.csv']:\n",
    "    upload_to_s3(csvfile, f\"merged_csvs/{csvfile}\")\n",
    "#     os.remove(csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-actress",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nz_historical_dfs = get_time_aggregated_dfs(\"s3://statsnz-covid-xmiles/merged_csvs/gdelt-nz-18-21.csv\")\n",
    "au_historical_dfs = get_time_aggregated_dfs(\"s3://statsnz-covid-xmiles/merged_csvs/gdelt-au-18-21.csv\")\n",
    "ca_historical_dfs = get_time_aggregated_dfs(\"s3://statsnz-covid-xmiles/merged_csvs/gdelt-ca-18-21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_hist_count = pd.concat({'nz': nz_historical_dfs[2], 'au': au_historical_dfs[2], 'ca': ca_historical_dfs[2]}, axis=1)\n",
    "\n",
    "daily_hist_count_long = daily_hist_count.reset_index() \\\n",
    "                              .melt(\"date\",\n",
    "                                    var_name=\"country\", \n",
    "                                    value_name=\"num_articles\")\n",
    "\n",
    "g = sns.lineplot(data=daily_hist_count_long, x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Daily number of news articles - historical\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_hist_count = pd.concat({'nz': nz_historical_dfs[2], 'au': au_historical_dfs[2], 'ca': ca_historical_dfs[2]}, axis=1)\n",
    "\n",
    "weekly_hist_count_long = daily_hist_count.reset_index() \\\n",
    "                              .melt(\"date\",\n",
    "                                    var_name=\"country\", \n",
    "                                    value_name=\"num_articles\")\n",
    "\n",
    "g = sns.lineplot(data=weekly_hist_count_long, x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Daily number of news articles - historical\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_hist_count_long['year'] = weekly_hist_count_long['date'].dt.year\n",
    "weekly_hist_count_long['daymonth'] = pd.to_datetime(2040*10000 + weekly_hist_count_long['date'].dt.month*100 + weekly_hist_count_long['date'].dt.day,\n",
    "                                                    format=\"%Y%m%d\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(24, 16), sharey=True)\n",
    "ax = axes.ravel()\n",
    "for axi in ax:\n",
    "    axi.xaxis.set_major_formatter(mpl.dates.DateFormatter(\"%d-%b\"))\n",
    "\n",
    "g_nz = sns.lineplot(data=weekly_hist_count_long[weekly_hist_count_long['country'] == 'nz'],\n",
    "                    x=\"daymonth\", y=\"num_articles\", hue=\"year\", ax=ax[0])\n",
    "g_nz.set(title=\"Weekly number of NZ news articles, by year\",\n",
    "         xlabel=\"\", ylabel=\"Number of articles\")\n",
    "\n",
    "g_au = sns.lineplot(data=weekly_hist_count_long[weekly_hist_count_long['country'] == 'au'],\n",
    "                    x=\"daymonth\", y=\"num_articles\", hue=\"year\", ax=ax[1])\n",
    "g_au.set(title=\"Weekly number of AU news articles, by year\", \n",
    "         xlabel=\"\", ylabel=\"Number of articles\")\n",
    "\n",
    "g_ca = sns.lineplot(data=weekly_hist_count_long[weekly_hist_count_long['country'] == 'ca'],\n",
    "                    x=\"daymonth\", y=\"num_articles\", hue=\"year\", ax=ax[1])\n",
    "g_ca.set(title=\"Weekly number of CA news articles, by year\", \n",
    "         xlabel=\"\", ylabel=\"Number of articles\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_hist_count_long['daymonth'].dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime({'year': itertools.repeat(2040), 'month': weekly_hist_count_long['date'].dt.month, 'day'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
