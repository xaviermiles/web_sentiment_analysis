{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assisted-exchange",
   "metadata": {},
   "source": [
    "# Country-wise comparison\n",
    "\n",
    "**V1.1 includes S3 integration, whereas the original notebook did not.**\n",
    "\n",
    "Comparison with New Zealand, Australia, and Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amended-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import boto3\n",
    "from s3fs.core import S3FileSystem\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7, 8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scenic-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session(profile_name=\"xmiles\")\n",
    "\n",
    "def upload_to_s3(in_fpath, out_key):\n",
    "    s3 = sess.client('s3')\n",
    "    bucket_name = 'statsnz-covid-xmiles'\n",
    "    \n",
    "    s3.put_object(Body=open(in_fpath, 'rb'), Bucket=bucket_name, Key=out_key)\n",
    "    \n",
    "\n",
    "def read_file_from_s3(bucket, key):\n",
    "    s3 = sess.client('s3')\n",
    "    \n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    content = obj['Body'].read().decode()\n",
    "    \n",
    "    return content\n",
    "\n",
    "def list_all_objects_s3(bucket, prefix):\n",
    "    \"\"\"\n",
    "    Necessary since the list_objects_v2() function only lists the first 1000 \n",
    "    objects, and requires a continuation token to get the next 1000 objects.\n",
    "    \"\"\"\n",
    "    s3 = sess.client('s3')\n",
    "    keys = []\n",
    "    truncated = True\n",
    "    next_cont_token = \"\"\n",
    "\n",
    "    while truncated:\n",
    "        if next_cont_token:\n",
    "            resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, \n",
    "                                      ContinuationToken=next_cont_token)\n",
    "        else:\n",
    "            resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        keys += [x['Key'] for x in resp['Contents'] \n",
    "                 if \".ipynb_checkpoints\" not in x['Key']]\n",
    "\n",
    "        truncated = resp['IsTruncated']\n",
    "        if truncated:\n",
    "            next_cont_token = resp['NextContinuationToken']\n",
    "            \n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consecutive-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csvs(mergefile, infpaths, overwrite=False):\n",
    "    if os.path.exists(mergefile) and not overwrite:\n",
    "        return\n",
    "    \n",
    "    with open(mergefile, 'w', newline=\"\") as outfile:\n",
    "        outwriter = csv.writer(outfile, delimiter=',')\n",
    "        outwriter.writerow(headers)\n",
    "        for fpath in infpaths:\n",
    "            with open(fpath) as infile:\n",
    "                inwriter = csv.reader(infile, delimiter=',')\n",
    "                outwriter.writerows(inwriter)\n",
    "                \n",
    "\n",
    "def merge_csvs_from_s3(mergefile, inkeys, bucket, headers, overwrite=False):\n",
    "    if os.path.exists(mergefile) and not overwrite:\n",
    "        return\n",
    "    \n",
    "    # Clear existing CSV\n",
    "    with open(mergefile, 'w') as f:\n",
    "        f.write('')\n",
    "    \n",
    "    for i, key in enumerate(inkeys):\n",
    "        content = read_csv_from_s3(bucket, key)\n",
    "        with open(mergefile, mode='a') as f:\n",
    "            f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-metabolism",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = [\n",
    "    'gkg_id', 'date', 'source', 'source_name', 'doc_id', \n",
    "    'themes', 'locations', 'persons', 'orgs', \n",
    "    'tone', 'pos', 'neg', 'polarity', 'ard', 'srd',\n",
    "    'wc', \n",
    "    'lexicode_neg', 'lexicode_pos', # c3.*\n",
    "    'MACROECONOMICS', 'ENERGY', 'FISHERIES', \n",
    "    'TRANSPORTATION', 'CRIME', 'SOCIAL_WELFARE',\n",
    "    'HOUSING', 'FINANCE', 'DEFENCE', 'SSTC',\n",
    "    'FOREIGN_TRADE', 'CIVIL_RIGHTS', \n",
    "    'INTL_AFFAIRS', 'GOVERNMENT_OPS',\n",
    "    'LAND-WATER-MANAGEMENT', 'CULTURE',\n",
    "    'PROV_LOCAL', 'INTERGOVERNMENTAL',\n",
    "    'CONSTITUTIONAL_NATL_UNITY', 'ABORIGINAL',\n",
    "    'RELIGION', 'HEALTHCARE', 'AGRICULTURE',\n",
    "    'FORESTRY', 'LABOUR', 'IMMIGRATION',\n",
    "    'EDUCATION', 'ENVIRONMENT',\n",
    "    'finstab_pos', 'finstab_neg', 'finstab_neutral',\n",
    "    'finsent_neg', 'finsent_pos', 'finsent_unc',\n",
    "    'opin_neg', 'opin_pos',\n",
    "    'sent_pos', 'sent_neg', 'sent_pol'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sought-plate",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_to_prefix = { \n",
    "    'nz': \"processed_gdelt_nz/\",\n",
    "    'au': \"processed_gdelt_au/\",\n",
    "    'ca': \"processed_gdelt_ca/\"\n",
    "}\n",
    "countries = ['nz', 'au', 'ca']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-salad",
   "metadata": {},
   "source": [
    "**Loading the merged 2020/2021 CSV files for New Zealand, Australia, and Canada exceeds the available RAM so jupyter crashes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "union-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_aggregated_dfs(csv_file):\n",
    "    \"\"\"\n",
    "    Returns four DataFrames for the given country\n",
    "    - daily_tone: tone, pos, neg (daily)\n",
    "    - weekly_tone: tone, pos, neg (weekly)\n",
    "    - daily_count: number of articles (daily)\n",
    "    - weekly_count: number of articles (weekly)\n",
    "    \"\"\"\n",
    "    gdelt = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(\"read\")\n",
    "    print(gdelt.head())\n",
    "    gdelt['date'] = pd.to_datetime(gdelt['date'], format=\"%Y%m%d%H%M%S\")\n",
    "    gdelt = gdelt.sort_values(by=[\"gkg_id\"]).reset_index()\n",
    "    print(\"tidied\")\n",
    "    \n",
    "    daily_tone = gdelt.resample('D', on='date')[['tone', 'pos', 'neg']].mean()\n",
    "    daily_count = gdelt.resample('D', on='date')['gkg_id'].count()\n",
    "    \n",
    "    weekly_tone = gdelt.resample('W-Mon', on='date')[['tone', 'pos', 'neg']].mean()\n",
    "    weekly_count = gdelt.resample('W-Mon', on='date')['gkg_id'].count()\n",
    "    # Remove partial weeks at beginning and end of weekly-aggregation\n",
    "#     first_monday = \n",
    "#     final_sunday = \n",
    "#     weekly_tone = weekly_tone[first_monday <= weekly_tone['date'] <= final_sunday]\n",
    "#     weekly_count = weekly_count[first_monday <= weekly_count['date'] <= final_sunday]\n",
    "    print(\"compiled\")\n",
    "    \n",
    "    return daily_tone, weekly_tone, daily_count, weekly_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f610a14-fefd-4301-854d-dfb57ed9b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_aggregated_dfs_s3(bucket, csv_key):\n",
    "    \"\"\"\n",
    "    Returns four DataFrames for the given country\n",
    "    - daily_tone: tone, pos, neg (daily)\n",
    "    - weekly_tone: tone, pos, neg (weekly)\n",
    "    - daily_count: number of articles (daily)\n",
    "    - weekly_count: number of articles (weekly)\n",
    "    \"\"\"\n",
    "    gdelt_raw = read_file_from_s3(bucket, csv_key)\n",
    "    gdelt = pd.DataFrame([\n",
    "        x for x in line.split(',')\n",
    "        for line in gdelt_raw.split('\\r\\n')\n",
    "    ])\n",
    "    \n",
    "    print(\"read\")\n",
    "    print(gdelt.head())\n",
    "    gdelt['date'] = pd.to_datetime(gdelt['date'], format=\"%Y%m%d%H%M%S\")\n",
    "    gdelt = gdelt.sort_values(by=[\"gkg_id\"]).reset_index()\n",
    "    print(\"tidied\")\n",
    "    \n",
    "    daily_tone = gdelt.resample('D', on='date')[['tone', 'pos', 'neg']].mean()\n",
    "    daily_count = gdelt.resample('D', on='date')['gkg_id'].count()\n",
    "    \n",
    "    weekly_tone = gdelt.resample('W-Mon', on='date')[['tone', 'pos', 'neg']].mean()\n",
    "    weekly_count = gdelt.resample('W-Mon', on='date')['gkg_id'].count()\n",
    "    # Remove partial weeks at beginning and end of weekly-aggregation\n",
    "#     first_monday = \n",
    "#     final_sunday = \n",
    "#     weekly_tone = weekly_tone[first_monday <= weekly_tone['date'] <= final_sunday]\n",
    "#     weekly_count = weekly_count[first_monday <= weekly_count['date'] <= final_sunday]\n",
    "    print(\"compiled\")\n",
    "    \n",
    "    return daily_tone, weekly_tone, daily_count, weekly_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-effects",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nz_dfs = get_time_aggregated_dfs('gdelt-nz-20-21.csv')\n",
    "au_dfs = get_time_aggregated_dfs('gdelt-au-20-21.csv')\n",
    "ca_dfs = get_time_aggregated_dfs('gdelt-ca-20-21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_columns = pd.MultiIndex.from_product([countries, ['tone', 'pos', 'neg']])\n",
    "\n",
    "daily_tone = pd.concat([nz_dfs[0], au_dfs[0], ca_dfs[0]], axis=1)\n",
    "daily_tone.columns = tone_columns\n",
    "\n",
    "weekly_tone = pd.concat([nz_dfs[1], au_dfs[1], ca_dfs[1]], axis=1)\n",
    "weekly_tone.columns = tone_columns\n",
    "\n",
    "daily_tone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_tone_long = daily_tone.xs('tone', axis=1, level=1) \\\n",
    "                            .reset_index() \\\n",
    "                            .melt(\"date\",\n",
    "                                  var_name=\"country\", \n",
    "                                  value_name=\"tone\", \n",
    "                                  value_vars=['nz','au','ca'])\n",
    "weekly_tone_long = weekly_tone.xs('tone', axis=1, level=1) \\\n",
    "                              .reset_index() \\\n",
    "                              .melt(\"date\",\n",
    "                                    var_name=\"country\", \n",
    "                                    value_name=\"tone\", \n",
    "                                    value_vars=['nz','au','ca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=daily_tone_long, x=\"date\", y=\"tone\", hue=\"country\")\n",
    "g.set(title=\"Daily tone of News (2020-present)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=weekly_tone_long, x=\"date\", y=\"tone\", hue=\"country\")\n",
    "g.set(title=\"Weekly tone of News (2020-present)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count = pd.concat({'nz': nz_dfs[2], 'au': au_dfs[2], 'ca': ca_dfs[2]}, axis=1)\n",
    "weekly_count = pd.concat({'nz': nz_dfs[3], 'au': au_dfs[3], 'ca': ca_dfs[3]}, axis=1)\n",
    "\n",
    "daily_count_long = daily_count.reset_index() \\\n",
    "                              .melt(\"date\",\n",
    "                                    var_name=\"country\", \n",
    "                                    value_name=\"num_articles\", \n",
    "                                    value_vars=['nz','au','ca'])\n",
    "weekly_count_long = weekly_count.reset_index() \\\n",
    "                                .melt(\"date\",\n",
    "                                      var_name=\"country\", \n",
    "                                      value_name=\"num_articles\", \n",
    "                                      value_vars=['nz','au','ca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=daily_count_long, x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Number of news articles (daily)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=weekly_count_long, x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Number of news articles (weekly)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-apparatus",
   "metadata": {},
   "source": [
    "## Verify that seasonality is weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count.index.strftime(\"%b %y\")\n",
    "['Jan 20', 'Mar 20', 'Jul 20', 'Oct 20', 'Jan 21', 'Mar 21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_trends = pd.concat({\n",
    "    country: seasonal_decompose(daily_count.dropna()[country],\n",
    "                                model='additive'\n",
    "                                ).trend\n",
    "    for country in countries\n",
    "}, axis=1)\n",
    "\n",
    "g = sns.lineplot(data=tone_trends, dashes=False)\n",
    "g.set(title=\"Daily number of articles - trend component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count['Day'] = daily_count.index.day_name().astype(\"category\").reorder_categories(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "first_monday = daily_count[daily_count['Day'] == \"Monday\"].index[0]\n",
    "first_monday_idx = (first_monday - datetime(2020, 1, 1)).days\n",
    "\n",
    "first_monday, first_monday_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_seasonals = pd.concat({\n",
    "    country: seasonal_decompose(daily_count.dropna()[country],#, 'tone'], \n",
    "                                model='additive'\n",
    "                                ).seasonal[first_monday_idx:(first_monday_idx+6)]\n",
    "    for country in countries\n",
    "}, axis=1)\n",
    "\n",
    "g = sns.lineplot(data=tone_seasonals, dashes=False)\n",
    "g.set(title=\"Daily number of articles - seasonal component (Mon - Sun)\",\n",
    "      xticks=[], xlabel='');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-twenty",
   "metadata": {},
   "source": [
    "# Check previous years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6db0dc2-ac64-454d-a978-0fb8559370ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "signed integer is greater than maximum",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8f9c687e2922>\u001b[0m in \u001b[0;36mget_time_aggregated_dfs_s3\u001b[0;34m(bucket, csv_key)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0mweekly_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0marticles\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweekly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgdelt_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file_from_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     gdelt = pd.DataFrame([\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d163c7538bea>\u001b[0m in \u001b[0;36mread_file_from_s3\u001b[0;34m(bucket, key)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mURLLib3ReadTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# TODO: the url will be None as urllib3 isn't setting it yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0;31m# cStringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m                 \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mIncompleteRead\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdetect\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: signed integer is greater than maximum"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nz_18_21_dfs = get_time_aggregated_dfs_s3(\"statsnz-covid-xmiles\", \"merged_gdelt/gdelt-nz-18-21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463b563-1656-40f5-8d2b-2a0960e90b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "au_18_21_dfs = get_time_aggregated_dfs(fs.open(\"s3://statsnz-covid-xmiles/merged_gdelt/gdelt-au-18-21.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8594da-e612-4009-93ea-fbb1f3be4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ca_18_21_dfs = get_time_aggregated_dfs(fs.open(\"s3://statsnz-covid-xmiles/merged_gdelt/gdelt-ca-18-21.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0082e71-81b6-401d-98e2-1bea7a4cfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_18_21_dfs2 = get_time_aggregated_dfs_s3(\"statsnz-covid-xmiles\", \"merged_gdelt/gdelt-nz-18-21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_hist_count = pd.concat({'nz': nz_18_21_dfs[2], 'au': au_18_21_dfs[2], 'ca': ca_18_21_dfs[2]}, axis=1)\n",
    "\n",
    "daily_hist_count_long = daily_hist_count.reset_index() \\\n",
    "                                        .melt(\"date\", var_name=\"country\",\n",
    "                                              value_name=\"num_articles\") \\\n",
    "                                        .dropna()\n",
    "\n",
    "g = sns.lineplot(data=daily_hist_count_long.dropna(), x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Daily number of news articles - historical\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_hist_count = pd.concat({'nz': nz_18_21_dfs[3], 'au': au_18_21_dfs[3], 'ca': ca_18_21_dfs[3]}, axis=1)\n",
    "\n",
    "weekly_hist_count_long = weekly_hist_count.reset_index() \\\n",
    "                                          .melt(\"date\", var_name=\"country\", \n",
    "                                                value_name=\"num_articles\") \\\n",
    "                                          .dropna()\n",
    "\n",
    "g = sns.lineplot(data=weekly_hist_count_long, x=\"date\", y=\"num_articles\", hue=\"country\")\n",
    "g.set(title=\"Weekly number of news articles - historical\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_hist_count_long['year'] = weekly_hist_count_long['date'].dt.year\n",
    "weekly_hist_count_long['daymonth'] = pd.to_datetime(2040*10000 + weekly_hist_count_long['date'].dt.month*100 + weekly_hist_count_long['date'].dt.day,\n",
    "                                                    format=\"%Y%m%d\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(24, 16), sharey=True)\n",
    "ax = axes.ravel()\n",
    "for axi in ax[:3]:\n",
    "    axi.xaxis.set_major_formatter(mpl.dates.DateFormatter(\"%d-%b\"))\n",
    "\n",
    "g_nz = sns.lineplot(data=weekly_hist_count_long[weekly_hist_count_long['country'] == 'nz'],\n",
    "                    x=\"daymonth\", y=\"num_articles\", hue=\"year\", ax=ax[0])\n",
    "g_nz.set(title=\"Weekly number of NZ news articles, by year\",\n",
    "         xlabel=\"\", ylabel=\"Number of articles\")\n",
    "\n",
    "g_au = sns.lineplot(data=weekly_hist_count_long[weekly_hist_count_long['country'] == 'au'],\n",
    "                    x=\"daymonth\", y=\"num_articles\", hue=\"year\", ax=ax[1])\n",
    "g_au.set(title=\"Weekly number of AU news articles, by year\", \n",
    "         xlabel=\"\", ylabel=\"Number of articles\")\n",
    "\n",
    "g_ca = sns.lineplot(data=weekly_hist_count_long[weekly_hist_count_long['country'] == 'ca'],\n",
    "                    x=\"daymonth\", y=\"num_articles\", hue=\"year\", ax=ax[2])\n",
    "g_ca.set(title=\"Weekly number of CA news articles, by year\", \n",
    "         xlabel=\"\", ylabel=\"Number of articles\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_hist_count_long['daymonth'].dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime({'year': itertools.repeat(2040), 'month': weekly_hist_count_long['date'].dt.month, 'day'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801d387-d4ca-453e-923c-c7fc086f0356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
